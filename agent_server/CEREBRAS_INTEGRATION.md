# Cerebras Integration - What Changed

## Summary

The W2L Agent Server now has **full Cerebras integration built-in**. The server handles all tool calling automatically, so clients just send requests and get back generated code.

## What Changed

### Before (Mock Implementation)

The original `server.js` was a mock:
- âŒ Didn't actually call Cerebras
- âŒ Returned fake responses
- âŒ Required external tool calling loop
- âŒ Client had to manage conversation state

The `cerebras-example.js` was doing all the work:
- Managing tool calling loop
- Executing tools
- Forwarding results back to Cerebras
- Continuing until completion

### After (Full Integration)

**Server (`server.js`)**
- âœ… Uses `@cerebras/cerebras_cloud_sdk`
- âœ… Loads API key from `../.env`
- âœ… Handles tool calling loop automatically
- âœ… Executes tools without client involvement
- âœ… Returns final generated code
- âœ… Works without API key (for testing endpoints)

**Client (`cerebras-example.js`)**
- âœ… Simplified to just call the server
- âœ… No more tool calling logic
- âœ… Just sends query and gets result

## Technical Changes

### 1. Dependencies Added

```json
{
  "@cerebras/cerebras_cloud_sdk": "^1.0.0",
  "dotenv": "^16.3.1"
}
```

### 2. Server Architecture

```javascript
// Load API key from project root .env
dotenv.config({ path: path.join(__dirname, '../.env') });

// Initialize Cerebras (only if API key available)
let cerebras = null;
if (process.env.CEREBRAS_API_KEY) {
  cerebras = new Cerebras({ apiKey: process.env.CEREBRAS_API_KEY });
}

// Chat endpoint with automatic tool calling
app.post('/v1/chat/completions', async (req, res) => {
  // ... prepare messages with system prompt
  
  // Tool calling loop
  while (iteration < maxIterations) {
    const completion = await cerebras.chat.completions.create({
      messages,
      model,
      tools: toolSchemas,
      tool_choice: 'auto'
    });
    
    // If tool calls, execute them
    if (assistantMessage.tool_calls) {
      const toolResults = await executeToolCalls(assistantMessage.tool_calls);
      messages.push(...toolResults);
      continue;  // Loop again
    }
    
    // No tool calls - return final result
    return res.json(completion);
  }
});
```

### 3. Automatic Tool Execution

```javascript
async function executeToolCalls(toolCalls) {
  const results = [];
  
  for (const toolCall of toolCalls) {
    const toolName = toolCall.function.name;
    const toolArgs = JSON.parse(toolCall.function.arguments);
    
    // Execute the tool
    const result = executeTool(toolName, toolArgs);
    
    // Format for Cerebras
    results.push({
      role: 'tool',
      tool_call_id: toolCall.id,
      content: JSON.stringify(result)
    });
  }
  
  return results;
}
```

### 4. System Prompt Builder

```javascript
function buildSystemPrompt(guides, elements) {
  // Creates comprehensive prompt with:
  // - Available guides
  // - Available elements by category
  // - Workflow instructions
  // - Code generation guidelines
}
```

## Usage Changes

### Before

**Client had to:**
1. Get context from server
2. Build system prompt
3. Call Cerebras API
4. Parse tool calls
5. Execute tools via server
6. Send results back to Cerebras
7. Repeat until done

**Example:**
```javascript
// 100+ lines of tool calling loop management
while (iteration < maxIterations) {
  const response = await callCerebras(messages, tools);
  if (response.tool_calls) {
    for (const toolCall of response.tool_calls) {
      const result = await executeTool(toolCall);
      messages.push(result);
    }
  } else {
    return response;
  }
}
```

### After

**Client just:**
```javascript
// One simple POST request
const response = await fetch('http://localhost:3100/v1/chat/completions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    messages: [{ role: 'user', content: 'Create a circle' }]
  })
});

const result = await response.json();
console.log(result.choices[0].message.content);
```

**That's it!** Server handles everything.

## Configuration

### Setup .env File

Create `.env` in project root (not in `agent_server/`):

```bash
CEREBRAS_API_KEY=your-api-key-here
```

The server looks for it at `../.env` (one level up from `agent_server/`).

### Environment Variables

- `CEREBRAS_API_KEY` - Your Cerebras API key (required for AI features)
- `PORT` - Server port (default: 3100)

## Testing

### Without API Key (Basic Testing)

```bash
cd agent_server
npm run dev

# Test endpoints
curl http://localhost:3100/health
curl http://localhost:3100/context
curl -X POST http://localhost:3100/tools/execute \
  -d '{"tool":"get_elements","arguments":{"elements":["Circle"]}}'
```

### With API Key (AI Testing)

```bash
# 1. Create .env in project root
echo "CEREBRAS_API_KEY=your-key" > .env

# 2. Start server
cd agent_server && npm run dev

# 3. Test AI
./test-ai.sh "Create a blue circle"
# or
node cerebras-example.js "Create a diagram"
```

## New Files

- `test-ai.sh` - Test script for AI functionality
- `CEREBRAS_INTEGRATION.md` - This file

## Updated Files

- `server.js` - Complete rewrite with Cerebras integration
- `cerebras-example.js` - Simplified to just call server
- `package.json` - Added Cerebras SDK and dotenv
- `README.md` - Updated with new usage
- `QUICKSTART.md` - Updated setup instructions
- `INTEGRATION.md` - Complete rewrite for new architecture

## API Model

**Default:** `llama-3.3-70b`

You can specify different models:

```bash
curl -X POST http://localhost:3100/v1/chat/completions \
  -d '{
    "messages": [...],
    "model": "llama-3.1-8b"  # or any Cerebras model
  }'
```

## Error Handling

### No API Key
```json
{
  "error": "CEREBRAS_API_KEY not configured. Please set it in .env file in the project root."
}
```

### Max Iterations
```json
{
  "choices": [{
    "message": {
      "content": "Maximum iterations reached. Please try again with a simpler request."
    }
  }]
}
```

### Tool Execution Error

If a tool fails, the error is passed back to Cerebras, which will either:
- Try a different approach
- Ask for clarification
- Return an error message

## Server Logs

The server logs all activity for debugging:

```
ðŸ¤– Chat Completion Request
   Model: llama-3.3-70b
   Messages: 1
   Stream: false

ðŸ”„ Iteration 1
   Agent requested 1 tool call(s)
   ðŸ”§ Executing: get_guides({"guides":["basic-shapes"]})
      âœ“ Success

ðŸ”„ Iteration 2
   Agent requested 1 tool call(s)
   ðŸ”§ Executing: get_elements({"elements":["Circle"]})
      âœ“ Success

ðŸ”„ Iteration 3
   âœ¨ Agent completed
```

## Benefits

1. **Simpler Clients** - Just POST and get result
2. **Centralized Logic** - Tool calling in one place
3. **Better Debugging** - Server logs show everything
4. **Easier Maintenance** - Update server, all clients benefit
5. **OpenAI Compatible** - Standard API format
6. **Stateless** - No conversation state in client

## Migration Guide

If you had custom code calling the old mock server:

### Before
```javascript
// Had to manage tool calling yourself
const context = await getContext();
const result = await manageToolCalling(prompt, context);
```

### After
```javascript
// Just call the server
const response = await fetch('http://localhost:3100/v1/chat/completions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    messages: [{ role: 'user', content: prompt }]
  })
});
const result = await response.json();
```

## Next Steps

1. âœ… Set up `.env` with your Cerebras API key
2. âœ… Run `npm run dev` to start server
3. âœ… Test with `./test-ai.sh "Create a circle"`
4. âœ… Integrate into your application
5. âœ… Add more guides as needed
6. âœ… Deploy to production

## Support

For issues:
- Check server logs for errors
- Verify API key is set correctly
- Test basic endpoints first (without AI)
- Check Cerebras API status

## Performance

- **Without tool calls:** ~2 seconds
- **With 1-2 tool calls:** ~5 seconds
- **With 3-4 tool calls:** ~8 seconds

The server limits to 10 iterations to prevent infinite loops.

## Summary

**The server is now a complete, self-contained AI agent.** You don't need to handle tool calling yourself - just send requests and receive generated code!

